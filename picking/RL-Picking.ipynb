{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray scale marks for cells\n",
    "visited_mark = 0.9\n",
    "flag_mark = 0.65\n",
    "agent_mark = 0.5\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    0: 'left',\n",
    "    1: 'up',\n",
    "    2: 'right',\n",
    "    3: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Tmaze(object):\n",
    "    \"\"\"\n",
    "    Tour De Flags maze object\n",
    "    maze: a 2d Numpy array of 0's and 1's\n",
    "        1.00 - a free cell\n",
    "        0.65 - flag cell\n",
    "        0.50 - agent cell\n",
    "        0.00 - an occupied cell\n",
    "    agent: (row, col) initial agent position (defaults to (0,0))\n",
    "    flags: list of cells occupied by flags\n",
    "    \"\"\"\n",
    "    def __init__(self, maze, flags, agent=(0,0), target=None):\n",
    "        self._maze = np.array(maze)\n",
    "        self._flags = set(flags)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        if target is None:\n",
    "            self.target = (nrows-1, ncols-1)   # default target cell where the agent to deliver the \"flags\"\n",
    "        self.free_cells = set((r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
    "        self.free_cells.discard(self.target)\n",
    "        self.free_cells -= self._flags\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not agent in self.free_cells:\n",
    "            raise Exception(\"Invalid agent Location: must sit on a free cell\")\n",
    "        self.reset(agent)\n",
    "\n",
    "    def reset(self, agent=(0,0)):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self._maze)\n",
    "        self.flags = set(self._flags)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.maze[row, col] = agent_mark\n",
    "        self.state = ((row, col), 'start')\n",
    "        self.base = np.sqrt(self.maze.size)\n",
    "        self.visited = dict(((r,c),0) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
    "        self.total_reward = 0\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.reward = {\n",
    "            'blocked':  self.min_reward,\n",
    "            'flag':     1.0/len(self._flags),\n",
    "            'invalid': -4.0/self.base,\n",
    "            'valid':   -1.0/self.maze.size\n",
    "        }\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        env_state = self.observe()\n",
    "        return env_state, reward, status\n",
    "\n",
    "    def get_reward(self):\n",
    "        agent, mode = self.state\n",
    "        if agent == self.target:\n",
    "            return 1.0 - len(self.flags) / len(self._flags)\n",
    "        if mode == 'blocked':\n",
    "            return self.reward['blocked']\n",
    "        elif agent in self.flags:\n",
    "            return self.reward['flag']\n",
    "        elif mode == 'invalid':\n",
    "            return self.reward['invalid']\n",
    "        elif mode == 'valid':\n",
    "            return self.reward['valid'] #* (1 + 0.1*self.visited[agent] ** 2)\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        (nrow, ncol), nmode = agent, mode = self.state\n",
    "\n",
    "        if self.maze[agent] > 0.0:\n",
    "            self.visited[agent] += 1  # mark visited cell\n",
    "        if agent in self.flags:\n",
    "            self.flags.remove(agent)\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == 0:    # move left\n",
    "                ncol -= 1\n",
    "            elif action == 1:  # move up\n",
    "                nrow -= 1\n",
    "            elif action == 2:    # move right\n",
    "                ncol += 1\n",
    "            elif action == 3:  # move down\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            nmode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        agent = (nrow, ncol)\n",
    "        self.state = (agent, nmode)\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        agent, mode = self.state\n",
    "        if agent == self.target:\n",
    "            if len(self.flags) == 0:\n",
    "                return 'win'\n",
    "            else:\n",
    "                return 'lose'\n",
    "\n",
    "        return 'ongoing'\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        env_state = canvas.reshape((1, -1))\n",
    "        return env_state\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the flags\n",
    "        for r,c in self.flags:\n",
    "            canvas[r,c] = flag_mark\n",
    "        # draw the agent\n",
    "        agent, mode = self.state\n",
    "        canvas[agent] = agent_mark\n",
    "        return canvas\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            (row, col), mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.97):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [env_state, action, reward, next_env_state, game_over]\n",
    "        # memory[i] = episode\n",
    "        # env_state == flattened 1d maze cells info, including agent cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, env_state):\n",
    "        return self.model.predict(env_state)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # env_state 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
    "            inputs[i] = env_state\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep (quote by Eder Santana)\n",
    "            targets[i] = self.predict(env_state)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(next_env_state))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qtraining(object):\n",
    "    def __init__(self, model, env, **opt):\n",
    "        self.model = model  # Nueral Network Model\n",
    "        self.env = env  # Environment (Tour De Flags maze object)\n",
    "        self.n_epoch = opt.get('n_epoch', 1000)  # Number of epochs to run\n",
    "        self.max_memory = opt.get('max_memory', 4*self.env.maze.size)  # Max memory for experiences\n",
    "        self.data_size = opt.get('data_size', int(0.75*self.env.maze.size))  # Data samples from experience replay\n",
    "        self.agent_cells = opt.get('agent_cells', [(0,0)])  # Starting cells for the agent\n",
    "        self.weights_file = opt.get('weights_file', \"\")  # Keras model weights file\n",
    "        self.name = opt.get('name', 'model')  # Name for saving weights and json files\n",
    "\n",
    "        self.win_count = 0\n",
    "        # If you want to continue training from a previous model,\n",
    "        # just supply the h5 file name to weights_file option\n",
    "        if self.weights_file:\n",
    "            print(\"loading weights from file: %s\" % (self.weights_file,))\n",
    "            self.model.load_weights(self.weights_file)\n",
    "\n",
    "        if self.agent_cells == 'all':\n",
    "            self.agent_cells = self.env.free_cells\n",
    "\n",
    "        # Initialize experience replay object\n",
    "        self.experience = Experience(self.model, max_memory=self.max_memory)\n",
    "\n",
    "    def train(self):\n",
    "        start_time = datetime.datetime.now()\n",
    "        self.seconds = 0\n",
    "        self.win_count = 0\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.epoch = epoch\n",
    "            self.loss = 0.0\n",
    "            agent = random.choice(self.agent_cells)\n",
    "            self.env.reset(agent)\n",
    "            game_over = False\n",
    "            # get initial env_state (1d flattened canvas)\n",
    "            self.env_state = self.env.observe()\n",
    "            self.n_episodes = 0\n",
    "            while not game_over:\n",
    "                game_over = self.play()\n",
    "\n",
    "            dt = datetime.datetime.now() - start_time\n",
    "            self.seconds = dt.total_seconds()\n",
    "            t = format_time(self.seconds)\n",
    "            fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | flags: {:d} | e: {:.3f} | time: {}\"\n",
    "            print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.flags), self.epsilon(), t))\n",
    "            if self.win_count > 2:\n",
    "                if self.completion_check():\n",
    "                    print(\"Completed training at epoch: %d\" % (epoch,))\n",
    "                    break\n",
    "\n",
    "    def play(self):\n",
    "        action = self.action()\n",
    "        prev_env_state = self.env_state\n",
    "        self.env_state, reward, game_status = self.env.act(action)\n",
    "        if game_status == 'win':\n",
    "            self.win_count += 1\n",
    "            game_over = True\n",
    "        elif game_status == 'lose':\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        # Store episode (experience)\n",
    "        episode = [prev_env_state, action, reward, self.env_state, game_over]\n",
    "        self.experience.remember(episode)\n",
    "        self.n_episodes += 1\n",
    "\n",
    "        # Train model\n",
    "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
    "        epochs = int(self.env.base)\n",
    "        h = self.model.fit(\n",
    "            inputs,\n",
    "            targets,\n",
    "            epochs = epochs,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
    "        return game_over\n",
    "\n",
    "    def run_game(self, agent):\n",
    "        self.env.reset(agent)\n",
    "        env_state = self.env.observe()\n",
    "        while True:\n",
    "            # get next action\n",
    "            q = self.model.predict(env_state)\n",
    "            action = np.argmax(q[0])\n",
    "            prev_env_state = env_state\n",
    "            # apply action, get rewards and new state\n",
    "            env_state, reward, game_status = self.env.act(action)\n",
    "            if game_status == 'win':\n",
    "                return True\n",
    "            elif game_status == 'lose':\n",
    "                return False\n",
    "\n",
    "    def action(self):\n",
    "        # Get next action\n",
    "        valid_actions = self.env.valid_actions()\n",
    "        if not valid_actions:\n",
    "            action = None\n",
    "        elif np.random.rand() < self.epsilon():\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            q = self.experience.predict(self.env_state)\n",
    "            action = np.argmax(q)\n",
    "        return action\n",
    "    \n",
    "    def run_game_save_img(self, agent):\n",
    "        self.env.reset(agent)\n",
    "        env_state = self.env.observe()\n",
    "        imgId = 0\n",
    "        while True:\n",
    "            imgId = imgId + 1\n",
    "            show_env(self.env, str(imgId)+'.png')\n",
    "            # get next action\n",
    "            q = self.model.predict(env_state)\n",
    "            action = np.argmax(q[0])\n",
    "            prev_env_state = env_state\n",
    "            # apply action, get rewards and new state\n",
    "            env_state, reward, game_status = self.env.act(action)\n",
    "            if game_status == 'win':\n",
    "                return True\n",
    "            elif game_status == 'lose':\n",
    "                return False\n",
    "\n",
    "    def epsilon(self):\n",
    "        n = self.win_count\n",
    "        top = 0.80\n",
    "        bottom = 0.08\n",
    "        if n<10:\n",
    "            e = bottom + (top - bottom) / (1 + 0.1 * n**0.5)\n",
    "        else:\n",
    "            e = bottom\n",
    "        return e\n",
    "    \n",
    "    def completion_check(self):\n",
    "        for agent in self.agent_cells:\n",
    "            if not self.run_game(agent):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def save(self, name=\"\"):\n",
    "        # Save trained model weights and architecture, this will be used by the visualization code\n",
    "        if not name:\n",
    "            name = self.name\n",
    "        h5file = 'model_%s.h5' % (name,)\n",
    "        json_file = 'model_%s.json' % (name,)\n",
    "        self.model.save_weights(h5file, overwrite=True)\n",
    "        with open(json_file, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "        t = format_time(self.seconds)\n",
    "        print('files: %s, %s' % (h5file, json_file))\n",
    "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env, **opt):\n",
    "    loss = opt.get('loss', 'mse')\n",
    "    a = opt.get('alpha', 0.24)\n",
    "    model = Sequential()\n",
    "    esize = env.maze.size\n",
    "    model.add(Dense(esize, input_shape=(esize,)))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(esize))\n",
    "    model.add(LeakyReLU(alpha=a))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_env(env, fname=None):\n",
    "    plt.grid('on')\n",
    "    n = env.maze.shape[0]\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, n, 1))\n",
    "    ax.set_yticks(np.arange(0.5, n, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(env.maze)\n",
    "    for cell in env.visited:\n",
    "        if env.visited[cell]:\n",
    "            canvas[cell] = visited_mark\n",
    "    for cell in env.flags:\n",
    "        canvas[cell] = flag_mark\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    return img\n",
    "\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eabd341cc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1., 1.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  0.,  0., 1.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  0.,  0., 1.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  0.,  0., 1.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  0.,  0., 1.],\n",
    "    [ 1.,  0.,  0.,  1.,  1.,  0.,  0., 1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1., 1.]\n",
    "])\n",
    "\n",
    "flags = [(6,1), (1,7)]\n",
    "\n",
    "env = Tmaze(maze, flags)\n",
    "show_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/199 | Loss: 0.0021 | Episodes:   51 | Wins:  0 | flags: 1 | e: 0.800 | time: 2.2 seconds\n",
      "Epoch:   1/199 | Loss: 0.0003 | Episodes:  296 | Wins:  0 | flags: 1 | e: 0.800 | time: 28.1 seconds\n",
      "Epoch:   2/199 | Loss: 0.0008 | Episodes:   57 | Wins:  0 | flags: 2 | e: 0.800 | time: 33.3 seconds\n",
      "Epoch:   3/199 | Loss: 0.0006 | Episodes:  212 | Wins:  1 | flags: 0 | e: 0.735 | time: 52.8 seconds\n",
      "Epoch:   4/199 | Loss: 0.0120 | Episodes:  211 | Wins:  2 | flags: 0 | e: 0.711 | time: 72.1 seconds\n",
      "Epoch:   5/199 | Loss: 0.0002 | Episodes:  104 | Wins:  2 | flags: 1 | e: 0.711 | time: 81.5 seconds\n",
      "Epoch:   6/199 | Loss: 0.0001 | Episodes:  104 | Wins:  3 | flags: 0 | e: 0.694 | time: 90.8 seconds\n",
      "Epoch:   7/199 | Loss: 0.0008 | Episodes:   23 | Wins:  3 | flags: 2 | e: 0.694 | time: 92.9 seconds\n",
      "Epoch:   8/199 | Loss: 0.0002 | Episodes:   99 | Wins:  4 | flags: 0 | e: 0.680 | time: 102.5 seconds\n",
      "Epoch:   9/199 | Loss: 0.0003 | Episodes:  172 | Wins:  5 | flags: 0 | e: 0.668 | time: 118.1 seconds\n",
      "Epoch:  10/199 | Loss: 0.0001 | Episodes:   98 | Wins:  6 | flags: 0 | e: 0.658 | time: 127.2 seconds\n",
      "Epoch:  11/199 | Loss: 0.0000 | Episodes:  141 | Wins:  7 | flags: 0 | e: 0.649 | time: 139.9 seconds\n",
      "Epoch:  12/199 | Loss: 0.0002 | Episodes:   53 | Wins:  8 | flags: 0 | e: 0.641 | time: 145.0 seconds\n",
      "Epoch:  13/199 | Loss: 0.0000 | Episodes:   71 | Wins:  9 | flags: 0 | e: 0.634 | time: 151.8 seconds\n",
      "Completed training at epoch: 13\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env)\n",
    "\n",
    "qt = Qtraining(\n",
    "    model,\n",
    "    env,\n",
    "    n_epoch = 200,\n",
    "    max_memory = 500,\n",
    "    data_size = 100,\n",
    "    name = 'model_1'\n",
    ")\n",
    "\n",
    "qt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qt.run_game_save_img((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
